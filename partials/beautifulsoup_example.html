<!DOCTYPE html>
<html>
<head>     <link href="prism.css" rel="stylesheet"> <script type="text/javascript-lazy"  >  	var admobid = {}; if( /(android)/i.test(navigator.userAgent) ) {     admobid = {            banner: 'ca-app-pub-5830283354936773/5921375320',        interstitial: 'ca-app-pub-5830283354936773/1902239267'     }; } else if(/(ipod|iphone|ipad)/i.test(navigator.userAgent)) {     admobid = {            banner: 'ca-app-pub-6869992474017983/4806197152',         interstitial: 'ca-app-pub-6869992474017983/7563979554'     }; } else {     admobid = {              banner: 'ca-app-pub-6869992474017983/8878394753',         interstitial: 'ca-app-pub-6869992474017983/1355127956'     }; } if(( /(ipad|iphone|ipod|android|windows phone)/i.test(navigator.userAgent) )) {     document.addEventListener('deviceready', initApp, false); } else {     initApp(); }  function initApp() {     if (! AdMob ) { alert( 'admob plugin not ready' ); return; }       AdMob.createBanner( {         adId: admobid.banner,         isTesting: false,         overlap: false,         offsetTopBar: false,         position: AdMob.AD_POSITION.BOTTOM_CENTER,         bgColor: 'black'     } );      AdMob.prepareInterstitial({         adId: admobid.interstitial,         autoShow: true     }); }    </script></head>
  <body style="background-color:white;" >
	
	<div class="topcoat-navigation-bar" ng-controller="HomeCtrl">
	  <div class="topcoat-navigation-bar__item left quarter">
		<a class="topcoat-icon-button--quiet" ng-click="slidePage('/','modal')">
		  <span class="topcoat-icon home-icon"></span>
		</a>
	  </div>
	  <div class="topcoat-navigation-bar__item center half">
		<h1 class="topcoat-navigation-bar__title">Beautiful Soup Example</h1>
	  </div>
	</div>
	
	<script type="text/javascript" src="prism.js"></script>

	<pre  ng-prism  class="language-python"><code>
		





### Beautiful Soup Example

from bs4 import BeautifulSoup # For using BeautifulSoup
from bs4 import UnicodeDammit # For using Unicode Converter 
import re # For Regular Expressions example

### HTML data that we'll parse (can be string or an actual .html file)
html_doc = """
&lthtml&gt&lthead&gt&lttitle&gtThe Dormouse's story&lt/title&gt&lt/head&gt
&ltbody&gt
&ltp class="title"&gt&ltb&gtThe Dormouse's story&lt/b&gt&lt/p&gt
&ltp class="story"&gtOnce upon a time there were three little sisters; and 
their names were:
&lta href="http://example.com/elsie" class="sister" id="link1"&gtElsie&lt/a&gt,
&lta href="http://example.com/lacie" class="sister" id="link2"&gtLacie&lt/a&gt and
&lta href="http://example.com/tillie" class="sister" id="link3"&gtTillie&lt/a&gt;
and they lived at the bottom of a well.</p>
&ltp class="story"&gt...&lt/p&gt
"""

def select_data_structure(mysoup):
    ### Examples of how to navigate the HTML structure with BS
    print "Title: ", mysoup.title # &lttitle&gtThe Dormouse's story&lt/title>&gt
    print "Title Name:", mysoup.title.name # title
    print "Title Parent Name:", mysoup.title.parent.name # head
    print "Paragraph:", mysoup.p
    # &ltp class="title"&gt&ltb&gtThe Doormouse's story&lt/b&gt&lt/p&gt
    print "Class of Paragraph:", mysoup.p['class'] # ['title']
    print "Links:", soup.a
    print "All Links:\n", soup.find_all('a')
    # [&lta class="sister" href="http://example.com/elise" id="link1"&gtElise&lt/a&gt
    # &lta class="sister" href="http://example.com/lacie" id="link2"&gtLacie&lt/a&gt
    # &lta class="sister" href="http://example.com/tillie" id="link3"&gtTillie&lt/a&gt]
    print "Find by ID:", soup.find(id="link3")
    # &lta class="sister" href="http://example.com/tillie" id="link3"&gtTillie&lt/a&gt


def common_tasks(mysoup):
    ### Extract all the URLs found within a page's <a> tags
    for link in soup.find_all('a'):
        print(link.get('href'))
    # http://example.com/elsie
    # http://example.com/lacie
    # http://example.com/tillie

    ### Extract all the text from a page
    print(soup.get_text())
    # The Dormouse's story
    #
    # The Dormouse's story
    # Once upon a time there were three little sisters; and
    # their names were:
    # Elsie,
    # Lacie and
    # Tillie;
    # and they lived at the bottom of a well.
    # ...


def tag_example():
    # A BeautifulSoup object represents the document as a whole
    # Treat as a Tag, supports most of the navigating/searching the tree methods

    # A Tag object corresponds to an XML or HTML tag in the original document
    soup = BeautifulSoup('&ltb class="boldest"&gtExtremely bold&lt/b&gt')
    mytag = soup.b # Make tag from b tags
    print "The Tag is:", mytag # &ltb class="boldest"&gtExtremely bold&ltb&gt
    print "Tag name:", mytag.name # b

    # A Tag object has many attributes, treat it like a dictionary
    print "Tag Attributes:", mytag.attrs # {'class': ['boldest']}
    print "Tag's class:", mytag['class'] #['boldest']

    # You can add, remove, and modify a tag's attributes
    mytag['class'] = 'verybold' # modify a tag
    mytag['id'] = 1 # add an attribute
    print "Added and modified attribute on Tag:", mytag
    del mytag['class']
    print "Deleted attribute on Tag:", mytag

    # Some HTML attributes can have multiple values (e.g. class)
    # Attribute 'class' can have more than one CSS class, in BS shows as a list
    css_soup = BeautifulSoup('&ltp class="body"&gt&lt/p&gt') # Single class
    print "Soup with single class:", css_soup.p['class'] # ['body']
    css_soup = BeautifulSoup('&ltp class="body strikeout"&gt&lt/p&gt') # Two classes
    print "Soup with two classes:", css_soup.p['class'] # ["body", "strikeout"]
 
    # If a tag attribute looks like multi-valued attributes, but its defined by
    # HTML standards as a single value, the attribute remains a single value
    id_soup = BeautifulSoup('&ltp id="my id"&gt&lt/p&gt')
    # ID is single attribute, but given as a multi-value
    print "Unknown Attribute Example:", id_soup.p['id'] #Keeps as single value

    # If parsing a document as XML, there are no multi-valued attributes
    xml_soup = BeautifulSoup('&ltp class="body strikeout"&gt&lt/p&gt', 'xml')
    print "Parsing XML always returns single value:", xml_soup.p['class']
    # [body strikeout] # See how it returns as one large string instead of list


def navigablestring_example(mysoup):
    # NavigableString class adds navigation to a String (allows up, down, etc.)

    soup = BeautifulSoup('&ltb class="boldest"&gtExtremely bold&lt/b&gt')
    mytag = soup.b # Make tag from b tags

    ### Strings correspond to text within a tag using NavigableString class
    # If only one string, then use .string; if two+ strings, use .strings

    # Example - Only one string in Tag, use .string
    print "\nString of Tag:", mytag.string # Extremely bold
    print "Type of tag.string:", type(mytag.string)
    # &ltclass 'bs4.element.NavigableString'
    print "Converting tag.string element back to Unicode string:", \
        unicode(mytag.string)
    print "Type of tag.string:", type(unicode(mytag.string)) # &lttype 'unicode'&gt

    # Example - 2+ strings in Tag, can iterate through using .strings
    print "Iterating through all strings using .strings"
    for string in mysoup.strings:
        print (repr(string))
    #\n
    #The Dormouse's story
    # ...

    # Example - 2+ strings in Tag, can iterate through using .stripped_strings
    # with the added benefit of ignore all whitespace
    print "Iterating through all strings using .stripped_strings"
    for string in mysoup.stripped_strings:
        print (repr(string))
    #The Dormouse's story
    #The Dormouse's story
    #Once upon a time there were three little sisters...

    ### Comment object is a subclass of the NavigableString class
    markup = "&ltb&gt<!--Hey, buddy.  Want to buy a used parser?--></b>"
    soup = BeautifulSoup(markup)
    comment = soup.b.string
    print "Comment Type:", type(comment) # &ltclass 'bs4.element.Comment'&gt
    print "Comment:", comment # Hey, buddy. Want to buy a used parser?


def navigate_data_structure_as_tree(mysoup):
    # How to navigate the parse tree of a Soup
    
    print "Select HTML tags (head):", mysoup.head
    # &lthead&gt&lttitle&gtThe Dormouse's story&lt/title&gt&lt/head&gt
    print "Zoom in on HTML tags (body.b):", mysoup.body.b 
    # &ltb&gtThe Dormouse's story&lt/b&gt
    print "Get all tags:", mysoup.find_all('a'), "\n" # Get all links

    # Get tag's children available through .contents and returns a list
    head_tag = mysoup.head
    print "\nTag's children (head):", head_tag.contents
    # [&lttitle&gtThe Dormouse's story&lt/title&gt]
    title_tag = head_tag.contents[0] #Contents are available as a list
    print "Tag's children (title)", title_tag.contents
    # [u"The Dormouse's story"]

    for child in head_tag.children:
        print(child)
    #&lttitle&gtThe Dormouse's story&lt/title&gt

    ### Can navigate down (.contents, .children, .descendants), 
    ### up (.parent, .parents) and sideways (.next_sibling, .previous_sibling)

    ### Can also navigate by CSS class
    # Can also use CSS selectors by passing in string to the .select() method
    mysoup.select("title")


def find_all_example(mysoup):
    # Find tags by filtering on a string, regular expression, list, or function
    
    ### Find with just a string
    print "Finding tags by string, using tag 'b':", mysoup.find_all('b')
    # [&ltb&gtThe Dormouse's story&lt/b&gt]
    print "Finding tags by string, using id:", mysoup.find_all(id='link2')
    # [&lta class="sister" href="http://example.com/lacie" id="link2">Lacie&lt/a&gt]

    ### Find with a regular expression (acts like a match() method)
    print "Finding tags by regex for tags starting with 'b':"
    for mytag in soup.find_all(re.compile("^b")):
        print(mytag.name) # Finds names that start with 'b'
    # body
    # b

    ### Find using a list, matches against any item in that list
    print "Finding with a list", soup.find_all(["a", "b"])
    # &ltb&gtThe Dormouse's story&lt/b&gt,
    # &lta class="sister" href="http://example.com/elsie", id="link1">Elsie&lt/a&gt

    ### Function (if the above methods don't work), element as the only
    # argument and should return True (if matches) or False (if no match)
    # See Documentation


def unicode_dammit_example():
    # Install the 'chardet' or 'cchardet' Python libraries for better guesses

    ### Take a string with unknown encoding and make the string Unicode
    weirdass_string = "Sacr\xc3\xa9 bleu!"
    dammit = UnicodeDammit(weirdass_string)
    print "Original Word with weird encoding:", weirdass_string
    print "Dammit Print:", (dammit.unicode_markup)
    print "Dammit Type:", (dammit.original_encoding)

    ### Take a doc with mostly UTF-8 encoding (and misc encodings due to mult
    # data sources) and convert to UTF-8 Unicode with .Dammit.detwingle()
    snowmen = (u"\N{SNOWMAN}" * 3)
    quote = (u"\N{LEFT DOUBLE QUOTATION MARK}I like snowmen!\N{RIGHT DOUBLE QUOTATION MARK}")
    doc = snowmen.encode("utf8") + quote.encode("windows-1252")
    # So now we have one doc with two encodings in it, printing is a mess
    #print "Weird Decoding doc with utf8:", doc # messed up, won't print
    #print (doc.decode("windows-1252")) # So messed up it doesn't even print

    # Decode using UnicodeDammit.detwingle() converts the string to pure UTF-8
    new_doc = UnicodeDammit.detwingle(doc)
    print new_doc.decode("utf8")

if __name__ == '__main__':
    
    ### Setup soup with string of HTML structure and defining a parser
    soup = BeautifulSoup(html_doc, "html.parser")
    # Multiple parsers including:
    #   "html.parser" is Python's HTML Parser
    #   'lxml' and 'xml' is library lxml's HTML and XML Parser
    #   'html5lib' is same parsing as a web browser, creates valid HTML5

    ### Print the contents of the soup (HTML) 
    print(soup.prettify() + "\n") # Print out the soup in pretty print

    ### Beautiful Soup transforms HTML doc into 4 kinds of objects:
    ### 1.) BeautifulSoup, 2.) Tag, 3.) NavigableString, 4.) Comment
    
    ### 1.) BeautifulSoup and 2.) Tag Example
    #tag_example()

    ### 3.) Navigable String and 4.) Comment Example
    #navigablestring_example(soup)

    ### How to select HTML tags
    #select_data_structure(soup) # Examples of how to select HTML structure
    
    ### Navigate the data structure as a tree (go up, down, sideways)
    #navigate_data_structure_as_tree(soup)

    ### Find all and filter by a string, a regular expression, list or function
    #find_all_example(soup)

    ### Commonly used tasks (for extracting data)
    #common_tasks(soup) # Commons tasks like extracting all URLS and text

    ### Unicode, Dammit is used whenever you want to convert unknown encoding
    # to straight Unicode
    unicode_dammit_example()
    
    
    
 </code></pre>
  </body>
</html>		
    
